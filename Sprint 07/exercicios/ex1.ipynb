{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Compilado de exercicios do Curso de Amazon Bedrock\n",
    "    Este compilado possui apenas uma copia dos scripts que foram executados na plataforma Lambda da AWS e nos arquivos .py, eles não foram executados neste notebook.\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seção 5: Criação de imagens utilizando Bedrock, API Gateway e S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#1. import boto3\n",
    "import boto3\n",
    "import base64\n",
    "import datetime\n",
    "\n",
    "#2. Criar conexão do cliente com os serviços Bedrock e S3 – Link  \n",
    "client_bedrock = boto3.client('bedrock-runtime')\n",
    "client_s3 = boto3.client('s3')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "#3. Armazenar os dados de entrada (prompt) em uma variável  \n",
    "    input_prompt=event['prompt']\n",
    "    print(input_prompt)\n",
    "\n",
    "#4. Criar uma sintaxe de requisição para acessar o serviço Bedrock  \n",
    "    response_bedrock = client_bedrock.invoke_model(contentType='application/json', accept='application/json',modelId='amazon.titan-image-generator-v2:0',\n",
    "    body =json.dumps( {\n",
    "        \"taskType\": \"TEXT_IMAGE\",\n",
    "        \"textToImageParams\": {\"text\": input_prompt,      \n",
    "        \"negativeText\": \"none\"\n",
    "    },\n",
    "            \"imageGenerationConfig\": {\n",
    "            \"quality\": \"standard\",\n",
    "            \"numberOfImages\": 1,\n",
    "            \"height\": 512,\n",
    "            \"width\": 512,\n",
    "            \"cfgScale\": 10.0,\n",
    "            \"seed\":42 \n",
    "                }\n",
    "    }))\n",
    "    \n",
    "    print(response_bedrock)\n",
    "\n",
    "#5. 5a. Recuperar do dicionário, 5b. Converter o corpo do streaming para bytes usando json load, 5c. Imprimir  \n",
    "    response_bedrock_byte=json.loads(response_bedrock['body'].read())\n",
    "    print(response_bedrock_byte)\n",
    "    \n",
    "#6. 6a. Recuperar dados com a chave do artefato, 6b. Importar Base64, 6c. Decodificar de Base64  \n",
    "    response_bedrock_base64 = response_bedrock_byte['images'][0]\n",
    "    response_bedrock_finalimage = base64.b64decode(response_bedrock_base64)\n",
    "    print(response_bedrock_finalimage)\n",
    "    \n",
    "#7. 7a. Fazer upload do arquivo para o S3 usando o método Put Object – Link  \n",
    "    poster_name = 'posterName'+ datetime.datetime.today().strftime('%Y-%M-%D-%M-%S')\n",
    "        response_s3=client_s3.put_object(\n",
    "        Bucket='posterfilme01',\n",
    "        Body=response_bedrock_finalimage,\n",
    "        Key=poster_name)\n",
    "\n",
    "#8. Gerar URL pré-assinada  \n",
    "    generate_presigned_url = client_s3.generate_presigned_url('get_object', Params={'Bucket':'posterfilme01','Key':poster_name}, ExpiresIn=3600)\n",
    "    print(generate_presigned_url)\n",
    "\n",
    "\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': generate_presigned_url #json.dumps('Hello from Lambda!')\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seção 6: Criação de Resumos utilizando Bedrock, API Gateway e Lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "# Criar cliente para o Bedrock\n",
    "client_bedrock = boto3.client('bedrock-runtime')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # Obter o input do evento\n",
    "    input_prompt = event['prompt']\n",
    "    # print(input_prompt)\n",
    "\n",
    "    # Criar a requisição para o Bedrock\n",
    "    client_bedrockrequest = client_bedrock.invoke_model(\n",
    "          modelId= \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "          contentType= \"application/json\",\n",
    "          accept= \"application/json\",\n",
    "          body= json.dumps({\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 400,\n",
    "            \"messages\": [\n",
    "              {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                  {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": input_prompt\n",
    "                  }\n",
    "                ]\n",
    "              }\n",
    "            ]\n",
    "          })\n",
    "        )\n",
    "     \n",
    "    # Ler e processar a resposta do Bedrock\n",
    "    client_bedrock_byte = client_bedrockrequest['body'].read()\n",
    "    client_bedrock_string = json.loads(client_bedrock_byte)\n",
    "\n",
    "    # Extrair a resposta do modelo\n",
    "    client_final_response2 = client_bedrock_string['content'][0]['text']\n",
    "    # client_final_response = client_bedrock_string.get('completion', 'Erro na resposta')\n",
    "\n",
    "    # print(client_final_response2)\n",
    "\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': (client_final_response2)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seção 7 - Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Steps\n",
    "#1 import the ConversationSummaryBufferMemory, ConversationChain, ChatBedrock (BedrockChat) Langchain Modules\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_aws import ChatBedrock\n",
    "#2a Write a function for invoking model- client connection with Bedrock with profile, model_id & Inference params- model_kwargs\n",
    "def demo_chatbot():\n",
    "    demo_llm=ChatBedrock(\n",
    "       credentials_profile_name='default',\n",
    "       model_id='anthropic.claude-3-haiku-20240307-v1:0',\n",
    "       model_kwargs= {\n",
    "           \"max_tokens\": 300,\n",
    "           \"temperature\": 0.1,\n",
    "           \"top_p\": 0.9,\n",
    "           \"stop_sequences\": [\"\\n\\nHuman:\"]} )\n",
    "    return demo_llm\n",
    "#2b Test out the LLM with Predict method instead use invoke method\n",
    "#     return demo_llm.invoke(input_text)\n",
    "# response=demo_chatbot(input_text=\"Hi, what is the temperature in new york in January?\")\n",
    "# print(response)\n",
    "\n",
    "# #3 Create a Function for  ConversationSummaryBufferMemory  (llm and max token limit)\n",
    "def demo_memory():\n",
    "    llm_data=demo_chatbot()\n",
    "    memory=ConversationSummaryBufferMemory(llm=llm_data,max_token_limit=300)\n",
    "    return memory\n",
    "\n",
    "#4 Create a Function for Conversation Chain - Input text + Memory\n",
    "def demo_conversation(input_text,memory):\n",
    "    llm_chain_data=demo_chatbot()\n",
    "    llm_conversation=ConversationChain(llm=llm_chain_data,memory=memory,verbose=True)\n",
    "\n",
    "#5 Chat response using invoke (Prompt template)\n",
    "    chat_reply=llm_conversation.invoke(input_text)\n",
    "    return chat_reply['response']\n",
    "\n",
    "# #1 https://python.langchain.com/v0.1/docs/integrations/llms/bedrock/\n",
    "# #pip install -U langchain-aws\n",
    "# #pip install anthropic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source:Below code is provided by Streamlit and AWS \n",
    "\n",
    "#1 import streamlit and chatbot file\n",
    "import streamlit as st \n",
    "import  chatbot_backend as demo  #**Import your Chatbot file as demo\n",
    "\n",
    "#2 Set Title for Chatbot - https://docs.streamlit.io/library/api-reference/text/st.title\n",
    "st.title(\"Hi, This is Chatbot Anisha :sunglasses:\") # **Modify this based on the title you want in want\n",
    "\n",
    "#3 LangChain memory to the session cache - Session State - https://docs.streamlit.io/library/api-reference/session-state\n",
    "if 'memory' not in st.session_state: \n",
    "    st.session_state.memory = demo.demo_memory() #** Modify the import and memory function() attributes initialize the memory\n",
    "\n",
    "#4 Add the UI chat history to the session cache - Session State - https://docs.streamlit.io/library/api-reference/session-state\n",
    "if 'chat_history' not in st.session_state: #see if the chat history hasn't been created yet\n",
    "    st.session_state.chat_history = [] #initialize the chat history\n",
    "\n",
    "#5 Re-render the chat history (Streamlit re-runs this script, so need this to preserve previous chat messages)\n",
    "for message in st.session_state.chat_history: \n",
    "    with st.chat_message(message[\"role\"]): \n",
    "        st.markdown(message[\"text\"]) \n",
    "\n",
    "#6 Enter the details for chatbot input box \n",
    "     \n",
    "input_text = st.chat_input(\"Powered by Bedrock and Claude\") # **display a chat input box\n",
    "if input_text: \n",
    "    \n",
    "    with st.chat_message(\"user\"): \n",
    "        st.markdown(input_text) \n",
    "    \n",
    "    st.session_state.chat_history.append({\"role\":\"user\", \"text\":input_text}) \n",
    "\n",
    "    chat_response = demo.demo_conversation(input_text=input_text, memory=st.session_state.memory) #** replace with ConversationChain Method name - call the model through the supporting library\n",
    "    \n",
    "    with st.chat_message(\"assistant\"): \n",
    "        st.markdown(chat_response) \n",
    "    \n",
    "    st.session_state.chat_history.append({\"role\":\"assistant\", \"text\":chat_response}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seção 8 - Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  # Biblioteca para manipulação de JSON\n",
    "import boto3  # Biblioteca para interagir com serviços AWS\n",
    "\n",
    "# Cria um cliente para o serviço Amazon Bedrock na região us-east-1\n",
    "client = boto3.client('bedrock-runtime', region_name='us-east-1')\n",
    "\n",
    "# ID do modelo da Amazon utilizado para gerar embeddings\n",
    "model_id = \"amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "# Texto de entrada para ser processado pelo modelo\n",
    "input_text = 'Hallo Welt!'\n",
    "\n",
    "# Cria o dicionário de requisição no formato esperado pela API\n",
    "native_request = {'inputText': input_text}\n",
    "\n",
    "# Converte o dicionário para uma string JSON\n",
    "request = json.dumps(native_request)\n",
    "\n",
    "# Envia a requisição para o modelo da Amazon Bedrock\n",
    "response = client.invoke_model(modelId=model_id, body=request)\n",
    "\n",
    "# Lê a resposta da API e converte de JSON para um dicionário Python\n",
    "model_response = json.loads(response['body'].read().decode('utf-8'))\n",
    "\n",
    "# Exibe a resposta completa para depuração\n",
    "print(\"Resposta completa do modelo:\", model_response)\n",
    "\n",
    "# Obtém os embeddings gerados pelo modelo (caso a chave exista, senão retorna uma lista vazia)\n",
    "embeddings = model_response.get('embedding', [])\n",
    "\n",
    "# Obtém a contagem de tokens de entrada (caso a chave exista, senão retorna um aviso)\n",
    "input_token_count = model_response.get('inputTokenCount', 'Chave não encontrada')\n",
    "\n",
    "# Exibe as informações processadas\n",
    "print(\"\\nSua entrada:\")\n",
    "print(input_text)\n",
    "print(f\"Número de tokens da entrada: {input_token_count}\")\n",
    "print(f\"Tamanho da geração de embedding: {len(embeddings)}\")\n",
    "print(\"Embedding:\")\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seção 9 - RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Import OS, Document Loader, Text Splitter, Bedrock Embeddings, Vector DB, VectorStoreIndex, Bedrock-LLM\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain_aws import BedrockLLM\n",
    "from langchain_aws import ChatBedrock \n",
    " \n",
    "#5c. Wrap within a function\n",
    "def hr_index():\n",
    "    #2. Define the data source and load data with PDFLoader(https://www.upl-ltd.com/images/people/downloads/Leave-Policy-India.pdf)\n",
    "    data_load=PyPDFLoader('https://www.upl-ltd.com/images/people/downloads/Leave-Policy-India.pdf')  \n",
    " \n",
    "    #3. Split the Text based on Character, Tokens etc. - Recursively split by character - [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    data_split=RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \" \", \"\"], chunk_size=100,chunk_overlap=10)\n",
    "    #4. Create Embeddings -- Client connection\n",
    "    data_embeddings=BedrockEmbeddings(\n",
    "    credentials_profile_name= 'default',\n",
    "    model_id='amazon.titan-embed-text-v2:0')\n",
    "    #5à Create Vector DB, Store Embeddings and Index for Search - VectorstoreIndexCreator\n",
    "    data_index=VectorstoreIndexCreator(\n",
    "        text_splitter=data_split,\n",
    "        embedding=data_embeddings,\n",
    "        vectorstore_cls=FAISS)\n",
    "    #5b  Create index for HR Policy Document\n",
    "    db_index=data_index.from_loaders([data_load])\n",
    "    return db_index\n",
    "#6a. Write a function to connect to Bedrock Foundation Model - Claude Foundation Model\n",
    "def hr_llm():\n",
    "    llm=ChatBedrock(\n",
    "        credentials_profile_name='default',\n",
    "        model_id= 'anthropic.claude-3-haiku-20240307-v1:0',#'anthropic.claude-v2',\n",
    "        model_kwargs={\n",
    "        \"max_tokens_to_sample\":3000,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.9})\n",
    "    return llm\n",
    "#6b. Write a function which searches the user prompt, searches the best match from Vector DB and sends both to LLM.\n",
    "def hr_rag_response(index,question):\n",
    "    rag_llm=hr_llm()\n",
    "    hr_rag_query=index.query(question=question,llm=rag_llm)\n",
    "    return hr_rag_query\n",
    "# Index creation --> https://api.python.langchain.com/en/latest/indexes/langchain.indexes.vectorstore.VectorstoreIndexCreator.htmlimport os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below frontend code is provided by AWS and Streamlit. I have only modified it to make it look attractive.\n",
    "import streamlit as st \n",
    "import rag_backend as demo ### replace rag_backend with your backend filename\n",
    "\n",
    "st.set_page_config(page_title=\"Pergunatas para o RH com RAG\") ### Modify Heading\n",
    "\n",
    "new_title = '<p style=\"font-family:sans-serif; color:Green; font-size: 42px;\"> Pergunatas para o RH com RAG 🎯</p>'\n",
    "st.markdown(new_title, unsafe_allow_html=True) ### Modify Title\n",
    "\n",
    "if 'vector_index' not in st.session_state: \n",
    "    with st.spinner(\"📀 Espere pela magia...Todas as coisas bonitas da vida levam tempo :-)\"): ###spinner message\n",
    "        st.session_state.vector_index = demo.hr_index() ### Your Index Function name from Backend File\n",
    "\n",
    "input_text = st.text_area(\"Input text\", label_visibility=\"collapsed\") \n",
    "go_button = st.button(\"📌Aprenda GenAI com Rahul Trisal\", type=\"primary\") ### Button Name\n",
    "\n",
    "if go_button: \n",
    "    \n",
    "    with st.spinner(\"📢Nascido para perder,viva para vencer. Lemmy Kilmister\"): ### Spinner message\n",
    "        response_content = demo.hr_rag_response(index=st.session_state.vector_index, question=input_text) ### replace with RAG Function from backend file\n",
    "        st.write(response_content) "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
